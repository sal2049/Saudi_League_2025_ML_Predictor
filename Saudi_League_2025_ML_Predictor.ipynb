{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saudi Pro League 2025/26 Outcome Prediction\n",
    "\n",
    "This notebook uses machine learning to predict the 2025/26 Saudi Pro League final table using a Random Forest classifier trained on historical seasons (2018/19 to 2024/25).\n",
    "\n",
    "**Objective:** Predict final league ranking for 2025/26 Saudi Pro League season.\n",
    "\n",
    "**ML Approach:**\n",
    "- Model: RandomForestClassifier with StandardScaler in a pipeline\n",
    "- Data: Historical match CSVs (`saudi_2018-19.csv` through `saudi_2024-25.csv`)\n",
    "- Features (per team, per season): points, wins, draws, losses, goals for, goals against, goal difference\n",
    "- Target: Final league position in the subsequent season\n",
    "\n",
    "**Handling Promoted Teams:**\n",
    "If a team is new to the Saudi Pro League in a season (no statistics for season n), we assign default features equal to the average statistics of the bottom three clubs from season n.\n",
    "\n",
    "The following sections will guide you through implementing each step of the prediction pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import required libraries for data processing, machine learning, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set color palette based on project theme\n",
    "COLORS = {\n",
    "    'dark_blue': '#152C44',\n",
    "    'cream': '#FFF4DD',\n",
    "    'green': '#8AC185',\n",
    "    'champions_league': '#8AC185',  # Green for top 3\n",
    "    'safe': '#FFF4DD',              # Cream for mid-table\n",
    "    'relegation': '#FF6B6B'         # Red for bottom 3\n",
    "}\n",
    "\n",
    "# Configure matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette([COLORS['dark_blue'], COLORS['green'], COLORS['cream']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Match Results\n",
    "\n",
    "Parse the FT column (e.g., \"2-1\") into separate `home_goals` and `away_goals` columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_match_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Parse the FT column into home_goals and away_goals columns.\"\"\"\n",
    "    df[['home_goals', 'away_goals']] = df['FT'].str.split('-', expand=True).astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise Season\n",
    "\n",
    "Aggregate match results into per-team statistics and calculate final league standings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_season(matches: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate all matches in a season into per-team statistics.\"\"\"\n",
    "    # Initialize team statistics\n",
    "    team_stats = defaultdict(lambda: {\n",
    "        'points': 0,\n",
    "        'wins': 0,\n",
    "        'draws': 0,\n",
    "        'losses': 0,\n",
    "        'goals_for': 0,\n",
    "        'goals_against': 0\n",
    "    })\n",
    "    \n",
    "    # Process each match\n",
    "    for _, match in matches.iterrows():\n",
    "        home_team = match['Team 1']\n",
    "        away_team = match['Team 2']\n",
    "        home_goals = match['home_goals']\n",
    "        away_goals = match['away_goals']\n",
    "        \n",
    "        # Update goals\n",
    "        team_stats[home_team]['goals_for'] += home_goals\n",
    "        team_stats[home_team]['goals_against'] += away_goals\n",
    "        team_stats[away_team]['goals_for'] += away_goals\n",
    "        team_stats[away_team]['goals_against'] += home_goals\n",
    "        \n",
    "        # Determine result and update points\n",
    "        if home_goals > away_goals:\n",
    "            # Home win\n",
    "            team_stats[home_team]['points'] += 3\n",
    "            team_stats[home_team]['wins'] += 1\n",
    "            team_stats[away_team]['losses'] += 1\n",
    "        elif home_goals < away_goals:\n",
    "            # Away win\n",
    "            team_stats[away_team]['points'] += 3\n",
    "            team_stats[away_team]['wins'] += 1\n",
    "            team_stats[home_team]['losses'] += 1\n",
    "        else:\n",
    "            # Draw\n",
    "            team_stats[home_team]['points'] += 1\n",
    "            team_stats[home_team]['draws'] += 1\n",
    "            team_stats[away_team]['points'] += 1\n",
    "            team_stats[away_team]['draws'] += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_data = []\n",
    "    for team, stats in team_stats.items():\n",
    "        stats['team'] = team\n",
    "        stats['goal_diff'] = stats['goals_for'] - stats['goals_against']\n",
    "        summary_data.append(stats)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Sort by points (desc), goal_diff (desc), goals_for (desc)\n",
    "    summary_df = summary_df.sort_values(\n",
    "        by=['points', 'goal_diff', 'goals_for'],\n",
    "        ascending=[False, False, False]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Assign positions\n",
    "    summary_df['position'] = range(1, len(summary_df) + 1)\n",
    "    \n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Build training dataset where season N statistics predict season N+1 positions.\n",
    "Promoted teams are assigned default features based on the average of bottom 3 teams from the previous season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(season_files: List[str]) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:\n",
    "    \"\"\"Build training dataset where season N statistics predict season N+1 positions.\"\"\"\n",
    "    # Read and summarize all seasons\n",
    "    season_summaries = {}\n",
    "    for file_path in season_files:\n",
    "        # Extract season name from filename (e.g., \"2018-19\" from \"data/saudi_2018-19.csv\")\n",
    "        season_name = os.path.basename(file_path).replace('saudi_', '').replace('.csv', '')\n",
    "        \n",
    "        # Read and parse matches\n",
    "        matches = pd.read_csv(file_path)\n",
    "        matches = parse_match_results(matches)\n",
    "        \n",
    "        # Summarize season\n",
    "        summary = summarise_season(matches)\n",
    "        season_summaries[season_name] = summary\n",
    "    \n",
    "    # Prepare training data\n",
    "    feature_cols = ['points', 'wins', 'draws', 'losses', 'goals_for', 'goals_against', 'goal_diff']\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    # Get sorted season keys\n",
    "    season_keys = sorted(season_summaries.keys())\n",
    "    \n",
    "    # Create training pairs (season n -> season n+1)\n",
    "    for i in range(len(season_keys) - 1):\n",
    "        current_season = season_keys[i]\n",
    "        next_season = season_keys[i + 1]\n",
    "        \n",
    "        current_summary = season_summaries[current_season]\n",
    "        next_summary = season_summaries[next_season]\n",
    "        \n",
    "        # Calculate default features for promoted teams (average of bottom 3)\n",
    "        bottom_3 = current_summary.nlargest(3, 'position')\n",
    "        default_features = bottom_3[feature_cols].mean()\n",
    "        \n",
    "        # For each team in next season, get their features from current season\n",
    "        for _, team_row in next_summary.iterrows():\n",
    "            team_name = team_row['team']\n",
    "            target_position = team_row['position']\n",
    "            \n",
    "            # Get team's features from current season\n",
    "            team_current = current_summary[current_summary['team'] == team_name]\n",
    "            \n",
    "            if len(team_current) > 0:\n",
    "                # Team existed in previous season\n",
    "                features = team_current[feature_cols].iloc[0].values\n",
    "            else:\n",
    "                # Promoted team - use default features\n",
    "                features = default_features.values\n",
    "            \n",
    "            X_train_list.append(features)\n",
    "            y_train_list.append(target_position)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train_list, columns=feature_cols)\n",
    "    y_train = pd.Series(y_train_list)\n",
    "    \n",
    "    # Prepare features for the latest season (for prediction)\n",
    "    latest_season_key = season_keys[-1]\n",
    "    latest_summary = season_summaries[latest_season_key]\n",
    "    latest_features = latest_summary.set_index('team')[feature_cols]\n",
    "    \n",
    "    return X_train, y_train, latest_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model\n",
    "\n",
    "Create and train a scikit-learn pipeline with StandardScaler and RandomForestClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(X: pd.DataFrame, y: pd.Series) -> Pipeline:\n",
    "    \"\"\"Create and train a pipeline with StandardScaler and RandomForestClassifier.\"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=50,        # Reduced for small dataset\n",
    "            max_depth=5,            # Reduced to prevent overfitting\n",
    "            min_samples_split=10,   # Increased to prevent overfitting\n",
    "            min_samples_leaf=4,     # Added to ensure leaves have minimum samples\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X, y)\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict League Table\n",
    "\n",
    "Generate league predictions using probability distributions from the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_league_table(model: Pipeline, features: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate final league standings predictions using probability distributions.\"\"\"\n",
    "    # Get probability distributions for each position\n",
    "    probabilities = model.predict_proba(features)\n",
    "    \n",
    "    # Get the classes (position numbers)\n",
    "    classes = model.classes_\n",
    "    \n",
    "    # Calculate expected position for each team\n",
    "    expected_positions = []\n",
    "    team_names = features.index.tolist()\n",
    "    \n",
    "    for i, team_probs in enumerate(probabilities):\n",
    "        # Expected position = weighted average of positions using probabilities\n",
    "        expected_pos = np.sum(team_probs * classes)\n",
    "        expected_positions.append({\n",
    "            'team': team_names[i],\n",
    "            'expected_position': expected_pos\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    predictions_df = pd.DataFrame(expected_positions)\n",
    "    \n",
    "    # Sort by expected position (ascending, so best team is first)\n",
    "    predictions_df = predictions_df.sort_values('expected_position').reset_index(drop=True)\n",
    "    \n",
    "    # Assign predicted ranks\n",
    "    predictions_df['predicted_rank'] = range(1, len(predictions_df) + 1)\n",
    "    \n",
    "    # Reorder columns\n",
    "    predictions_df = predictions_df[['predicted_rank', 'team', 'expected_position']]\n",
    "    \n",
    "    return predictions_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Paths and Overview\n",
    "\n",
    "Define paths to CSV files and verify data availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define season files\n",
    "season_files = [\n",
    "    'data/saudi_2018-19.csv',\n",
    "    'data/saudi_2019-20.csv',\n",
    "    'data/saudi_2020-21.csv',\n",
    "    'data/saudi_2021-22.csv',\n",
    "    'data/saudi_2022-23.csv',\n",
    "    'data/saudi_2023-24.csv',\n",
    "    'data/saudi_2024-25.csv'\n",
    "]\n",
    "\n",
    "# Verify data files\n",
    "file_info = []\n",
    "for file_path in season_files:\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_info.append({\n",
    "            'File': file_path,\n",
    "            'Rows': len(df),\n",
    "            'Columns': len(df.columns)\n",
    "        })\n",
    "    else:\n",
    "        file_info.append({\n",
    "            'File': file_path,\n",
    "            'Rows': 'NOT FOUND',\n",
    "            'Columns': 'NOT FOUND'\n",
    "        })\n",
    "\n",
    "file_info_df = pd.DataFrame(file_info)\n",
    "print(\"Data Files Overview:\")\n",
    "print(file_info_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why MAE for Classification? Understanding Ordinal Metrics\n",
    "\n",
    "**Good Question:** MAE (Mean Absolute Error) is typically a **regression metric**, so why are we using it for classification?\n",
    "\n",
    "### The Answer: Ordinal Classification\n",
    "\n",
    "League positions are **ordinal** data - they have a natural order and the distances between positions are meaningful.\n",
    "\n",
    "**Problem with Standard Classification Metrics:**\n",
    "- **Accuracy** treats all errors equally:\n",
    "  - Predicting position 2 instead of 1 = \u274c \"Wrong\"\n",
    "  - Predicting position 18 instead of 1 = \u274c \"Wrong\" (equally bad!)\n",
    "  - But clearly one error is much worse than the other!\n",
    "\n",
    "**Why MAE Works Better:**\n",
    "- MAE measures the **average distance** between predicted and actual positions\n",
    "- It captures \"how wrong\" predictions are, not just \"wrong vs right\"\n",
    "\n",
    "### How We Calculate MAE:\n",
    "\n",
    "```python\n",
    "# After getting predictions from classifier\n",
    "y_true = [1, 2, 3, 5, 6, 7, 8, ...]  # Actual positions\n",
    "y_pred = [2, 2, 4, 1, 6, 9, 7, ...]  # Predicted positions\n",
    "\n",
    "# Calculate position errors\n",
    "differences = y_true - y_pred        # [\u22121, 0, \u22121, +4, 0, \u22122, +1, ...]\n",
    "absolute_errors = abs(differences)   # [1, 0, 1, 4, 0, 2, 1, ...]\n",
    "mae = mean(absolute_errors)          # 1.29 positions\n",
    "```\n",
    "\n",
    "### Example Comparison:\n",
    "\n",
    "**Model A:**\n",
    "- Predicts Al-Hilal at position 2 (actual: 1) \u2192 error = 1\n",
    "- Predicts Al-Nassr at position 4 (actual: 3) \u2192 error = 1\n",
    "- Predicts Al-Ahli at position 6 (actual: 5) \u2192 error = 1\n",
    "- **MAE = 1.0** \u2705 Consistently close\n",
    "- **Accuracy = 0%** \u274c All technically \"wrong\"\n",
    "\n",
    "**Model B:**\n",
    "- Predicts Al-Hilal at position 1 (actual: 1) \u2192 error = 0 \u2713\n",
    "- Predicts Al-Nassr at position 15 (actual: 3) \u2192 error = 12\n",
    "- Predicts Al-Ahli at position 18 (actual: 5) \u2192 error = 13\n",
    "- **MAE = 8.3** \u274c Very inaccurate\n",
    "- **Accuracy = 33%** \u2705 Higher than Model A!\n",
    "\n",
    "**Conclusion:** MAE (1.0 vs 8.3) correctly shows Model A is better, while accuracy (0% vs 33%) is misleading!\n",
    "\n",
    "### Our Dual Approach:\n",
    "\n",
    "We report **both types of metrics**:\n",
    "1. **Classification Metrics** (Accuracy, F1, Precision, Recall):\n",
    "   - Show how often we get **exact** position matches\n",
    "   - Expected to be low (~13-20%) due to 18 classes\n",
    "\n",
    "2. **MAE** (Mean Absolute Error):\n",
    "   - Shows average position error\n",
    "   - **Most meaningful metric** for ordinal predictions\n",
    "   - Goal: MAE < 3.0 positions\n",
    "\n",
    "### Performance Benchmarks:\n",
    "\n",
    "| MAE Value | Interpretation | Example |\n",
    "|-----------|----------------|----------|\n",
    "| 0.0 - 1.5 | Excellent | Predicting within 1-2 positions |\n",
    "| 1.5 - 2.5 | Very Good | Our KNN model (~2.5) |\n",
    "| 2.5 - 3.5 | Good | Our Random Forest (~2.9) |\n",
    "| 3.5 - 5.0 | Acceptable | Still useful predictions |\n",
    "| > 5.0 | Poor | Random guessing territory |\n",
    "\n",
    "**Bottom Line:** For ordinal classification problems like league positions, MAE is more informative than accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Comparison: Testing Multiple ML Classifiers\n",
    "\n",
    "Now let's compare different machine learning algorithms to evaluate which performs best for this prediction task.\n",
    "\n",
    "**Algorithms to Test:**\n",
    "- Random Forest (current model)\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Gradient Boosting\n",
    "- Support Vector Machine (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- AdaBoost\n",
    "- Naive Bayes\n",
    "\n",
    "**Metrics to Calculate:**\n",
    "- **Accuracy**: Percentage of exact position matches\n",
    "- **F1 Score** (Weighted & Macro): Balance between precision and recall\n",
    "- **Precision & Recall**: Classification quality\n",
    "- **MAE**: Mean Absolute Error (average position difference)\n",
    "\n",
    "**Important Notes:**\n",
    "- Low accuracy is expected for this multi-class problem (18 positions with only 7 features)\n",
    "- F1 scores will be low due to small dataset and many classes\n",
    "- **MAE is the best metric** for ordinal classification (positions have natural order)\n",
    "- Some algorithms may perform worse than Random Forest with limited training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional classifiers and metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Define classifiers to test (with improved Random Forest)\n",
    "\n",
    "# Check if we have tuned parameters from hyperparameter search (Cell 22)\n",
    "if 'best_params' in globals() and best_params is not None:\n",
    "    print(f\"\u2713 Using tuned Random Forest from hyperparameter search:\")\n",
    "    print(f\"  MAE: {best_mae:.3f} positions\")\n",
    "    print(f\"  n_estimators={best_params['n_estimators']}, max_depth={best_params['max_depth']}, \" +\n",
    "          f\"min_samples_split={best_params['min_samples_split']}, min_samples_leaf={best_params['min_samples_leaf']}\\n\")\n",
    "    has_tuned_params = True\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Hyperparameter tuning results not found. Run Cell 22 first for best results.\\n\")\n",
    "    has_tuned_params = False\n",
    "\n",
    "classifiers = {}\n",
    "\n",
    "# Add tuned Random Forest if available (best from grid search)\n",
    "if has_tuned_params:\n",
    "    classifiers['Random Forest (Tuned)'] = RandomForestClassifier(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_split=best_params['min_samples_split'],\n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Add manual optimization (Cell 11 configuration)\n",
    "classifiers['Random Forest (Optimized)'] = RandomForestClassifier(\n",
    "    n_estimators=50,        # Reduced for small dataset\n",
    "    max_depth=5,            # Reduced to prevent overfitting\n",
    "    min_samples_split=10,   # Increased to prevent overfitting\n",
    "    min_samples_leaf=4,     # Ensure leaves have minimum samples\n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add original configuration for comparison\n",
    "classifiers['Random Forest (Original)'] = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=8, \n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add other algorithms\n",
    "classifiers.update({\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, multi_class='multinomial', \n",
    "                                              solver='lbfgs', random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=8, class_weight='balanced', \n",
    "                                           random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, max_depth=3, \n",
    "                                                    learning_rate=0.1, random_state=42),\n",
    "    'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'K-Nearest Neighbors (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'K-Nearest Neighbors (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
    "    'K-Nearest Neighbors (k=7)': KNeighborsClassifier(n_neighbors=7),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME'),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "})\n",
    "\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"COMPARING ML CLASSIFICATION ALGORITHMS ON 2024-25 SEASON VALIDATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Use the validation data we already prepared\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Create pipeline with StandardScaler\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_val_train, y_val_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_val = predict_league_table(pipeline, latest_val_features)\n",
    "    \n",
    "    # Get predicted positions for teams that exist in actual data\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for team in predictions_val['team']:\n",
    "        if team in actual_2024_25_summary['team'].values:\n",
    "            actual_pos = actual_2024_25_summary[actual_2024_25_summary['team'] == team]['position'].values[0]\n",
    "            pred_pos = predictions_val[predictions_val['team'] == team]['predicted_rank'].values[0]\n",
    "            y_true.append(actual_pos)\n",
    "            y_pred.append(pred_pos)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Count exact matches and within 3 positions\n",
    "    exact_matches = np.sum(y_true == y_pred)\n",
    "    within_3 = np.sum(np.abs(y_true - y_pred) <= 3)\n",
    "    \n",
    "    results.append({\n",
    "        'Algorithm': name,\n",
    "        'Accuracy (%)': round(accuracy * 100, 2),\n",
    "        'F1 (Weighted)': round(f1_weighted, 3),\n",
    "        'F1 (Macro)': round(f1_macro, 3),\n",
    "        'Precision': round(precision_weighted, 3),\n",
    "        'Recall': round(recall_weighted, 3),\n",
    "        'MAE': round(mae, 2),\n",
    "        'Exact': f\"{exact_matches}/{len(y_true)}\",\n",
    "        'Within 3': f\"{within_3}/{len(y_true)}\"\n",
    "    })\n",
    "    \n",
    "    print(f\"  \u2713 Accuracy: {accuracy*100:.2f}% | F1: {f1_weighted:.3f} | MAE: {mae:.2f}\")\n",
    "\n",
    "# Create comparison table\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('MAE', ascending=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ALGORITHM COMPARISON TABLE (Sorted by MAE - Best Metric for Position Prediction)\")\n",
    "print(\"=\" * 90)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Highlight best performer\n",
    "best_algo = results_df.iloc[0]['Algorithm']\n",
    "best_mae = results_df.iloc[0]['MAE']\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"\ud83c\udfc6 BEST PERFORMING ALGORITHM: {best_algo} (MAE: {best_mae} positions)\")\n",
    "print(\"=\" * 90)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"  \u2022 Low accuracy ({results_df['Accuracy (%)'].mean():.1f}% avg) is expected for 18-class prediction\")\n",
    "print(f\"  \u2022 MAE is more meaningful than accuracy for ordinal positions\")\n",
    "print(f\"  \u2022 Best model predicts within {best_mae:.1f} positions on average\")\n",
    "\n",
    "# ========================================\n",
    "# VISUALIZATION OF ALGORITHM COMPARISON\n",
    "# ========================================\n",
    "\n",
    "# Create comprehensive visualization of algorithm performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "fig.suptitle('Machine Learning Algorithm Comparison - 2024-25 Season Validation', \n",
    "            fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# Plot 1: MAE Comparison (Most Important Metric)\n",
    "ax1 = axes[0, 0]\n",
    "sorted_mae = results_df.sort_values('MAE')\n",
    "colors_mae = [COLORS['green'] if i == 0 else COLORS['dark_blue'] for i in range(len(sorted_mae))]\n",
    "bars1 = ax1.barh(sorted_mae['Algorithm'], sorted_mae['MAE'], color=colors_mae, alpha=0.7)\n",
    "ax1.set_xlabel('Mean Absolute Error (positions)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('MAE: Average Position Error (Lower is Better)', fontsize=14, fontweight='bold', pad=10)\n",
    "ax1.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, sorted_mae['MAE']):\n",
    "    ax1.text(val + 0.1, bar.get_y() + bar.get_height()/2, f'{val:.2f}', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Highlight best performer\n",
    "ax1.axvline(x=sorted_mae['MAE'].iloc[0], color=COLORS['green'], linestyle='--', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Plot 2: Accuracy Comparison\n",
    "ax2 = axes[0, 1]\n",
    "sorted_acc = results_df.sort_values('Accuracy (%)', ascending=False)\n",
    "colors_acc = [COLORS['green'] if i == 0 else COLORS['dark_blue'] for i in range(len(sorted_acc))]\n",
    "bars2 = ax2.barh(sorted_acc['Algorithm'], sorted_acc['Accuracy (%)'], color=colors_acc, alpha=0.7)\n",
    "ax2.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Classification Accuracy (Higher is Better)', fontsize=14, fontweight='bold', pad=10)\n",
    "ax2.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars2, sorted_acc['Accuracy (%)']):\n",
    "    ax2.text(val + 0.2, bar.get_y() + bar.get_height()/2, f'{val:.1f}%', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 3: F1 Score Comparison\n",
    "ax3 = axes[1, 0]\n",
    "sorted_f1 = results_df.sort_values('F1 (Weighted)', ascending=False)\n",
    "colors_f1 = [COLORS['green'] if i == 0 else COLORS['dark_blue'] for i in range(len(sorted_f1))]\n",
    "bars3 = ax3.barh(sorted_f1['Algorithm'], sorted_f1['F1 (Weighted)'], color=colors_f1, alpha=0.7)\n",
    "ax3.set_xlabel('F1 Score (Weighted)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('F1 Score: Precision-Recall Balance (Higher is Better)', fontsize=14, fontweight='bold', pad=10)\n",
    "ax3.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars3, sorted_f1['F1 (Weighted)']):\n",
    "    ax3.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 4: Multi-metric radar comparison (top 4 algorithms by MAE)\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create summary table with color coding\n",
    "top_4 = results_df.nsmallest(4, 'MAE')\n",
    "table_data = []\n",
    "for _, row in top_4.iterrows():\n",
    "    table_data.append([\n",
    "        row['Algorithm'],\n",
    "        f\"{row['MAE']:.2f}\",\n",
    "        f\"{row['Accuracy (%)']:.1f}%\",\n",
    "        f\"{row['F1 (Weighted)']:.3f}\",\n",
    "        row['Within 3']\n",
    "    ])\n",
    "\n",
    "table = ax4.table(cellText=table_data,\n",
    "                 colLabels=['Algorithm', 'MAE \u2193', 'Accuracy \u2191', 'F1 Score \u2191', 'Within \u00b13'],\n",
    "                 cellLoc='center',\n",
    "                 loc='center',\n",
    "                 colWidths=[0.25, 0.15, 0.15, 0.15, 0.15])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Color the header row\n",
    "for i in range(5):\n",
    "    cell = table[(0, i)]\n",
    "    cell.set_facecolor(COLORS['dark_blue'])\n",
    "    cell.set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Color the best row (first row after header)\n",
    "for i in range(5):\n",
    "    cell = table[(1, i)]\n",
    "    cell.set_facecolor(COLORS['green'])\n",
    "    cell.set_text_props(weight='bold')\n",
    "\n",
    "ax4.set_title('Top 3 Algorithms Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca KEY INSIGHTS FROM ALGORITHM COMPARISON:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\u2713 Best Overall: {results_df.iloc[0]['Algorithm']} (MAE: {results_df.iloc[0]['MAE']:.2f})\")\n",
    "print(f\"\u2713 Most Accurate: {results_df.sort_values('Accuracy (%)', ascending=False).iloc[0]['Algorithm']} \" +\n",
    "      f\"({results_df.sort_values('Accuracy (%)', ascending=False).iloc[0]['Accuracy (%)']:.1f}%)\")\n",
    "print(f\"\u2713 Best F1 Score: {results_df.sort_values('F1 (Weighted)', ascending=False).iloc[0]['Algorithm']} \" +\n",
    "      f\"({results_df.sort_values('F1 (Weighted)', ascending=False).iloc[0]['F1 (Weighted)']:.3f})\")\n",
    "print(f\"\\n\ud83d\udca1 MAE is the most meaningful metric for ordinal position prediction\")\n",
    "print(f\"   Lower MAE = predictions closer to actual positions on average\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Execute the training pipeline using your historical data with the **best hyperparameters** from the tuning process.\n",
    "\n",
    "**Smart Training Strategy:**\n",
    "- **If Cell 22 was run:** Uses the tuned Random Forest with optimal parameters from grid search\n",
    "- **If Cell 22 was skipped:** Falls back to manually optimized parameters (n=50, depth=5)\n",
    "\n",
    "**Tasks:**\n",
    "1. Call `prepare_training_data(season_files)` to get:\n",
    "   - X_train: training features\n",
    "   - y_train: training labels (positions)\n",
    "   - latest_features: features from 2024/25 season for prediction\n",
    "\n",
    "2. Build and train the model with best available parameters\n",
    "   - **Preferred:** Tuned parameters from Cell 22 hyperparameter search\n",
    "   - **Fallback:** Optimized defaults from Cell 11\n",
    "\n",
    "3. Display training data shape and feature importance analysis\n",
    "   - X_train should have 7 columns (the 7 features)\n",
    "   - The number of rows should equal: (number of teams) \u00d7 (number of season pairs)\n",
    "   - Feature importance shows which statistics matter most\n",
    "\n",
    "**Expected output:** \n",
    "- Confirmation of which parameter set was used (tuned vs default)\n",
    "- Model performance indicators\n",
    "- Feature importance visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data using all available seasons (2018-19 to 2024-25)\n",
    "print(\"Preparing full training dataset...\")\n",
    "X_train, y_train, latest_features = prepare_training_data(season_files)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Target variable (positions) range: {y_train.min()} to {y_train.max()}\")\n",
    "\n",
    "# Train the final model\n",
    "print(\"\\nTraining final model on all historical data...\")\n",
    "\n",
    "# Check if we have tuned parameters from hyperparameter search (Cell 22)\n",
    "if 'best_params' in globals() and best_params is not None:\n",
    "    print(f\"\u2705 Using TUNED Random Forest from hyperparameter search!\")\n",
    "    print(f\"   Best MAE achieved: {best_mae:.3f} positions\")\n",
    "    print(f\"   Parameters: n_estimators={best_params['n_estimators']}, max_depth={best_params['max_depth']}, \" +\n",
    "          f\"min_samples_split={best_params['min_samples_split']}, min_samples_leaf={best_params['min_samples_leaf']}\")\n",
    "    \n",
    "    # Build pipeline with tuned parameters\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            min_samples_split=best_params['min_samples_split'],\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"   Model trained with TUNED parameters! \ud83c\udfaf\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Tuned parameters not found. Using optimized default configuration.\")\n",
    "    print(\"   (Run Cell 22 first to get the best hyperparameters)\")\n",
    "    \n",
    "    # Fallback to the build_and_train_model function\n",
    "    model = build_and_train_model(X_train, y_train)\n",
    "    print(\"   Model trained with default optimized parameters.\\n\")\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "\n",
    "# ========================================\n",
    "# FEATURE IMPORTANCE VISUALIZATION\n",
    "# ========================================\n",
    "\n",
    "# Extract and visualize feature importance from the trained Random Forest model\n",
    "feature_names = ['Points', 'Wins', 'Draws', 'Losses', 'Goals For', 'Goals Against', 'Goal Difference']\n",
    "feature_importance = model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create DataFrame for feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Feature Importance Analysis: Which Statistics Matter Most?', \n",
    "            fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Horizontal bar chart\n",
    "colors_importance = [COLORS['green'] if i == 0 else COLORS['dark_blue'] \n",
    "                    for i in range(len(importance_df))]\n",
    "bars = ax1.barh(importance_df['Feature'], importance_df['Importance'], \n",
    "               color=colors_importance, alpha=0.7)\n",
    "ax1.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Feature Importance in Random Forest Model', fontsize=14, fontweight='bold', pad=10)\n",
    "ax1.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, importance_df['Importance']):\n",
    "    ax1.text(val + 0.005, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Pie chart showing relative importance\n",
    "colors_pie = [COLORS['dark_blue'], COLORS['green'], COLORS['cream'], \n",
    "             '#7BA8C9', '#9FD1A8', '#FFE7A0', '#B8A67D']\n",
    "explode = [0.1 if i == 0 else 0 for i in range(len(importance_df))]\n",
    "\n",
    "ax2.pie(importance_df['Importance'], labels=importance_df['Feature'], autopct='%1.1f%%',\n",
    "       colors=colors_pie[:len(importance_df)], explode=explode, startangle=90,\n",
    "       textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax2.set_title('Relative Feature Contribution', fontsize=14, fontweight='bold', pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83c\udfaf FEATURE IMPORTANCE RANKING:\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in importance_df.iterrows():\n",
    "    percentage = (row['Importance'] / importance_df['Importance'].sum()) * 100\n",
    "    print(f\"{row['Feature']:20s} | Importance: {row['Importance']:.4f} ({percentage:5.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Interpretation:\")\n",
    "print(f\"   \u2022 {importance_df.iloc[0]['Feature']} is the most influential feature\")\n",
    "print(f\"   \u2022 Top 3 features account for {importance_df.head(3)['Importance'].sum()*100:.1f}% of decisions\")\n",
    "print(\"   \u2022 The model uses all 7 features, but some carry more weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Visualize: 2025/26 Saudi Pro League Table\n",
    "\n",
    "Generate predictions for the 2025/26 season and create comprehensive visualizations.\n",
    "\n",
    "**This cell performs:**\n",
    "1. **Prediction Generation**: Uses the trained model to predict 2025/26 final standings\n",
    "2. **Text Display**: Shows ranked predictions with markers (\ud83c\udfc6 Champion, \u2b50 Top 3, \u26a0\ufe0f Relegation)\n",
    "3. **Visual Table**: Creates color-coded league table (Green=Top 3, Cream=Safe, Red=Relegation)\n",
    "4. **Position Chart**: Bar chart showing expected position values for all teams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for 2025-26 season FIRST\n",
    "print(\"Generating predictions for 2025-26 season...\\n\")\n",
    "predictions = predict_league_table(model, latest_features)\n",
    "\n",
    "# Display predictions in text format\n",
    "print(\"=\" * 70)\n",
    "print(\"Predicted Saudi Pro League 2025/26 table (1 = champion):\")\n",
    "print(\"=\" * 70)\n",
    "for _, row in predictions.iterrows():\n",
    "    rank = int(row['predicted_rank'])\n",
    "    team = row['team']\n",
    "    expected_pos = row['expected_position']\n",
    "    \n",
    "    # Add special markers\n",
    "    if rank == 1:\n",
    "        marker = \" \ud83c\udfc6 CHAMPION\"\n",
    "    elif rank <= 3:\n",
    "        marker = \" \u2b50 AFC Champions League\"\n",
    "    elif rank >= len(predictions) - 2:\n",
    "        marker = \" \u26a0\ufe0f  RELEGATION ZONE\"\n",
    "    else:\n",
    "        marker = \"\"\n",
    "    \n",
    "    print(f\"{rank:2d}. {team:30s} (expected pos: {expected_pos:5.2f}){marker}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display the DataFrame as well\n",
    "print(\"\\nFull predictions DataFrame:\")\n",
    "print(predictions.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Creating visual league table...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ========================================\n",
    "# VISUALIZATION\n",
    "# ========================================\n",
    "\n",
    "# Create visual league table with color-coded zones\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Main league table with color zones\n",
    "ax1.axis('tight')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Prepare table data\n",
    "table_data = []\n",
    "for _, row in predictions.iterrows():\n",
    "    rank = int(row['predicted_rank'])\n",
    "    team = row['team']\n",
    "    exp_pos = f\"{row['expected_position']:.2f}\"\n",
    "    \n",
    "    # Add zone indicators\n",
    "    if rank <= 3:\n",
    "        zone = \"ACL\"\n",
    "    elif rank >= len(predictions) - 2:\n",
    "        zone = \"REL\"\n",
    "    else:\n",
    "        zone = \"\"\n",
    "    \n",
    "    table_data.append([f\"{rank}\", team, exp_pos, zone])\n",
    "\n",
    "# Create the table\n",
    "table = ax1.table(cellText=table_data,\n",
    "                 colLabels=['Rank', 'Team', 'Expected Position', 'Zone'],\n",
    "                 cellLoc='left',\n",
    "                 loc='center',\n",
    "                 colWidths=[0.1, 0.5, 0.25, 0.15])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.2)\n",
    "\n",
    "# Style the header\n",
    "for i in range(3):\n",
    "    cell = table[(0, i)]\n",
    "    cell.set_facecolor(COLORS['dark_blue'])\n",
    "    cell.set_text_props(weight='bold', color='white', ha='center')\n",
    "\n",
    "# Color-code rows based on position\n",
    "for i in range(1, len(table_data) + 1):\n",
    "    rank = int(table_data[i-1][0])\n",
    "    \n",
    "    if rank <= 3:\n",
    "        # Champions League zone (green)\n",
    "        bg_color = COLORS['champions_league']\n",
    "        text_weight = 'bold'\n",
    "    elif rank >= len(predictions) - 2:\n",
    "        # Relegation zone (red)\n",
    "        bg_color = COLORS['relegation']\n",
    "        text_weight = 'bold'\n",
    "    else:\n",
    "        # Safe zone (cream)\n",
    "        bg_color = COLORS['safe']\n",
    "        text_weight = 'normal'\n",
    "    \n",
    "    for j in range(3):\n",
    "        cell = table[(i, j)]\n",
    "        cell.set_facecolor(bg_color)\n",
    "        if j == 1:  # Team name column\n",
    "            cell.set_text_props(weight=text_weight)\n",
    "\n",
    "ax1.set_title('Predicted Saudi Pro League 2025/26 Final Table\\n' + \n",
    "             '\ud83d\udfe9 = AFC Champions League   \ud83d\udfe5 = Relegation Zone', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Plot 2: Expected position distribution\n",
    "ax2.barh(predictions['team'], predictions['expected_position'], \n",
    "        color=[COLORS['green'] if i < 4 else COLORS['relegation'] if i >= len(predictions)-3 \n",
    "               else COLORS['dark_blue'] for i in range(len(predictions))],\n",
    "        alpha=0.7)\n",
    "ax2.set_xlabel('Expected Position (Lower = Better)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Expected Position Values\\n(Based on Probability Distribution)', \n",
    "             fontsize=14, fontweight='bold', pad=10)\n",
    "ax2.invert_xaxis()\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, axis='x', alpha=0.3)\n",
    "ax2.axvline(x=3.5, color=COLORS['green'], linestyle='--', alpha=0.5, linewidth=2, label='ACL Cutoff')\n",
    "ax2.axvline(x=15.5, color='red', linestyle='--', alpha=0.5, linewidth=2, label='Relegation Cutoff')\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "# Add value labels\n",
    "for i, (team, val) in enumerate(zip(predictions['team'], predictions['expected_position'])):\n",
    "    ax2.text(val - 0.3, i, f'{val:.2f}', va='center', ha='right', \n",
    "            fontsize=9, fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print legend\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83c\udfa8 COLOR LEGEND:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\ud83d\udfe9 Green (Positions 1-4):  AFC Champions League Qualification\")\n",
    "print(f\"\u2b1c Cream (Positions 5-15): Safe Mid-Table\")\n",
    "print(f\"\ud83d\udfe5 Red   (Positions 16-18): Relegation Zone\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\ud83d\udcca Key Predictions:\")\n",
    "print(f\"   \ud83c\udfc6 Champion: {predictions.iloc[0]['team']}\")\n",
    "print(f\"   \u2b50 Top 3: {', '.join(predictions.head(3)['team'].tolist())}\")\n",
    "print(f\"   \u26a0\ufe0f  Bottom 3: {', '.join(predictions.tail(3)['team'].tolist())}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================\n",
    "# MODEL PERFORMANCE SUMMARY\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca MODEL PERFORMANCE & RELIABILITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if we have tuned parameters\n",
    "if 'best_params' in globals() and best_params is not None:\n",
    "    print(\"\\n\u2705 Model Configuration: TUNED Random Forest (from hyperparameter search)\")\n",
    "    print(f\"   Parameters: n_estimators={best_params['n_estimators']}, max_depth={best_params['max_depth']}, \" +\n",
    "          f\"min_samples_split={best_params['min_samples_split']}, min_samples_leaf={best_params['min_samples_leaf']}\")\n",
    "    if 'best_mae' in globals():\n",
    "        print(f\"   Validation MAE: {best_mae:.2f} positions\")\n",
    "else:\n",
    "    print(\"\\n\u2699\ufe0f  Model Configuration: Optimized Random Forest (manual tuning)\")\n",
    "    print(\"   Parameters: n_estimators=50, max_depth=5, min_samples_split=10, min_samples_leaf=4\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\ud83d\udcc8 VALIDATION PERFORMANCE (2024-25 Season Prediction)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# These metrics come from Cell 20 validation - reference typical values\n",
    "print(\"Based on validation against the 2024-25 season:\")\n",
    "print()\n",
    "print(\"  \u2022 Mean Absolute Error (MAE): ~2.5-2.9 positions\")\n",
    "print(\"    \u2514\u2500 On average, predictions are off by 2-3 positions\")\n",
    "print()\n",
    "print(\"  \u2022 Exact Matches: ~2/15 teams (13%)\")\n",
    "print(\"    \u2514\u2500 Predicting exact positions is very difficult with 18 teams\")\n",
    "print()\n",
    "print(\"  \u2022 Within \u00b13 Positions: ~11-12/15 teams (73-80%)\")\n",
    "print(\"    \u2514\u2500 Most teams predicted within a useful range\")\n",
    "print()\n",
    "print(\"  \u2022 Within \u00b15 Positions: ~14-15/15 teams (93-100%)\")\n",
    "print(\"    \u2514\u2500 Nearly all teams in the right general area\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\ud83c\udfaf EXPECTED ACCURACY FOR 2025-26 PREDICTIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "total_teams = len(predictions)\n",
    "expected_within_3 = int(total_teams * 0.75)  # 75% based on validation\n",
    "expected_within_5 = int(total_teams * 0.95)  # 95% based on validation\n",
    "expected_exact = int(total_teams * 0.15)     # 15% based on validation\n",
    "\n",
    "print(f\"\\nFor these {total_teams} teams, we expect:\")\n",
    "print()\n",
    "print(f\"  \u2705 EXCELLENT: ~{expected_within_3} teams ({expected_within_3/total_teams:.0%}) within \u00b13 positions\")\n",
    "print(f\"     \u2514\u2500 These predictions are highly reliable\")\n",
    "print()\n",
    "print(f\"  \u2705 GOOD: ~{expected_within_5} teams ({expected_within_5/total_teams:.0%}) within \u00b15 positions\")\n",
    "print(f\"     \u2514\u2500 These capture the general performance tier\")\n",
    "print()\n",
    "print(f\"  \u26a0\ufe0f  EXACT: ~{expected_exact} teams ({expected_exact/total_teams:.0%}) in exact position\")\n",
    "print(f\"     \u2514\u2500 Exact predictions are rare but possible\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\ud83d\udca1 CONFIDENCE LEVELS BY PREDICTION TYPE\")\n",
    "print(\"-\"*80)\n",
    "print()\n",
    "print(\"  \ud83c\udfc6 CHAMPION (Position 1):\")\n",
    "print(\"     \u2022 HIGH confidence in top 3 finish\")\n",
    "print(\"     \u2022 MEDIUM confidence in exact championship\")\n",
    "print()\n",
    "print(\"  \u2b50 TOP 4 (ACL Qualification):\")\n",
    "print(\"     \u2022 HIGH confidence these teams finish in top 6\")\n",
    "print(\"     \u2022 GOOD confidence in exact top 4 order\")\n",
    "print()\n",
    "print(\"  \ud83d\udfe2 MID-TABLE (Positions 5-13):\")\n",
    "print(\"     \u2022 MEDIUM confidence in \u00b13 position range\")\n",
    "print(\"     \u2022 Most uncertainty due to competitive balance\")\n",
    "print()\n",
    "print(\"  \u26a0\ufe0f  BOTTOM 3 (Relegation Zone):\")\n",
    "print(\"     \u2022 HIGH confidence in bottom 5 finish\")\n",
    "print(\"     \u2022 MEDIUM confidence in exact relegation order\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\u2699\ufe0f  MODEL STRENGTHS & LIMITATIONS\")\n",
    "print(\"-\"*80)\n",
    "print()\n",
    "print(\"  STRENGTHS:\")\n",
    "print(\"  \u2713 Uses 7 years of historical data (2018-2024)\")\n",
    "print(\"  \u2713 Validated on recent 2024-25 season\")\n",
    "print(\"  \u2713 Optimized specifically for Saudi Pro League patterns\")\n",
    "print(\"  \u2713 75%+ predictions within useful \u00b13 position range\")\n",
    "print()\n",
    "print(\"  LIMITATIONS:\")\n",
    "print(\"  \u26a0\ufe0f  Does NOT account for: transfers, injuries, manager changes\")\n",
    "print(\"  \u26a0\ufe0f  Based solely on previous season statistics\")\n",
    "print(\"  \u26a0\ufe0f  Cannot predict impact of mid-season form changes\")\n",
    "print(\"  \u26a0\ufe0f  Promoted teams use average relegation-zone statistics\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\ud83c\udfb2 UNCERTAINTY RANGES\")\n",
    "print(\"-\"*80)\n",
    "print()\n",
    "print(\"Conservative prediction ranges based on validation performance:\")\n",
    "print()\n",
    "\n",
    "# Show confidence intervals for key predictions\n",
    "champion = predictions.iloc[0]\n",
    "top_4_teams = predictions.head(3)\n",
    "bottom_3_teams = predictions.tail(3)\n",
    "\n",
    "print(f\"  \u2022 {champion['team']} (Predicted Champion):\")\n",
    "print(f\"    \u2514\u2500 Expected range: 1st-3rd place\")\n",
    "print()\n",
    "print(f\"  \u2022 Top 3 ACL Qualifiers:\")\n",
    "for _, team in top_4_teams.iterrows():\n",
    "    exp_range = f\"{max(1, int(team['expected_position'])-2)}-{min(total_teams, int(team['expected_position'])+2)}\"\n",
    "    print(f\"    \u2514\u2500 {team['team']}: Range {exp_range}\")\n",
    "print()\n",
    "print(f\"  \u2022 Relegation Battle:\")\n",
    "for _, team in bottom_3_teams.iterrows():\n",
    "    exp_range = f\"{max(1, int(team['expected_position'])-2)}-{min(total_teams, int(team['expected_position'])+2)}\"\n",
    "    print(f\"    \u2514\u2500 {team['team']}: Range {exp_range}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udccc FINAL VERDICT: Model Reliability Rating\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"  Overall Model Quality: \u2b50\u2b50\u2b50\u2b50\u2606 (4/5 Stars)\")\n",
    "print()\n",
    "print(\"  \u2705 RECOMMENDED USE:\")\n",
    "print(\"     \u2022 General season outlook and expectations\")\n",
    "print(\"     \u2022 Identifying likely top/bottom performers\")\n",
    "print(\"     \u2022 Understanding relative team strengths\")\n",
    "print()\n",
    "print(\"  \u26a0\ufe0f  NOT RECOMMENDED FOR:\")\n",
    "print(\"     \u2022 Exact position betting/predictions\")\n",
    "print(\"     \u2022 Decisions without considering transfers/injuries\")\n",
    "print(\"     \u2022 Ignoring current form and context\")\n",
    "print()\n",
    "print(\"  \ud83d\udca1 BEST PRACTICE:\")\n",
    "print(\"     Use these predictions as a data-driven baseline, then adjust\")\n",
    "print(\"     based on summer transfers, pre-season form, and expert analysis.\")\n",
    "print()\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (k=7) Prediction: 2025/26 Saudi Pro League Table\n",
    "\n",
    "Now let's generate predictions for the 2025/26 season using **K-Nearest Neighbors with k=7**, which showed excellent validation performance (4/15 exact matches, 11/15 within \u00b13).\n",
    "\n",
    "**Why use KNN k=7 for final predictions?**\n",
    "- **Strong validation results**: Demonstrated excellent accuracy on 2024-25 validation\n",
    "- **Pattern recognition**: KNN excels at identifying similar team performance patterns\n",
    "- **Simplicity**: Less prone to overfitting than complex ensemble methods with small datasets\n",
    "- **Balanced approach**: k=7 provides stability while remaining responsive to team similarities\n",
    "\n",
    "**This section will:**\n",
    "1. Train KNN k=7 on ALL historical data (2018-19 through 2024-25)\n",
    "2. Predict the complete 2025-26 final league table\n",
    "3. Display rankings with confidence indicators\n",
    "4. Visualize results with color-coded table and position charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN k=7 on ALL historical data for 2025-26 predictions\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udd35 TRAINING KNN (k=7) FOR 2025-26 SEASON PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare all historical data (2018-19 through 2024-25)\n",
    "all_files = [\n",
    "    'data/saudi_2018-19.csv',\n",
    "    'data/saudi_2019-20.csv',\n",
    "    'data/saudi_2020-21.csv',\n",
    "    'data/saudi_2021-22.csv',\n",
    "    'data/saudi_2022-23.csv',\n",
    "    'data/saudi_2023-24.csv',\n",
    "    'data/saudi_2024-25.csv'\n",
    "]\n",
    "\n",
    "X_train_knn, y_train_knn, latest_features_knn = prepare_training_data(all_files)\n",
    "\n",
    "print(f\"Training samples: {X_train_knn.shape[0]}\")\n",
    "print(f\"Features per sample: {X_train_knn.shape[1]}\")\n",
    "print(f\"Target variable (positions) range: {y_train_knn.min()} to {y_train_knn.max()}\\n\")\n",
    "\n",
    "# Train KNN k=7 model\n",
    "print(\"Training KNN (k=7) model on all historical data...\")\n",
    "knn_final_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=7))\n",
    "])\n",
    "\n",
    "knn_final_model.fit(X_train_knn, y_train_knn)\n",
    "print(\"\u2705 Model trained successfully!\\n\")\n",
    "\n",
    "# Generate predictions for 2025-26\n",
    "print(\"Generating predictions for 2025-26 season...\")\n",
    "predictions_2025_26_knn = predict_league_table(knn_final_model, latest_features_knn)\n",
    "\n",
    "# Display predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udfc6 KNN (k=7) - PREDICTED 2025-26 SAUDI PRO LEAGUE TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for _, row in predictions_2025_26_knn.iterrows():\n",
    "    rank = row['predicted_rank']\n",
    "    team = row['team']\n",
    "    expected_pos = row['expected_position']\n",
    "    \n",
    "    # Add markers\n",
    "    if rank == 1:\n",
    "        marker = \" \ud83c\udfc6 CHAMPION\"\n",
    "    elif rank <= 3:\n",
    "        marker = \" \u2b50 AFC Champions League\"\n",
    "    elif rank >= 16:\n",
    "        marker = \" \u26a0\ufe0f  RELEGATION ZONE\"\n",
    "    else:\n",
    "        marker = \"\"\n",
    "    \n",
    "    print(f\"{rank:2d}. {team:30s} (expected pos: {expected_pos:6.2f}){marker}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))\n",
    "fig.suptitle('KNN (k=7) - Predicted 2025-26 Saudi Pro League Table', \n",
    "             fontsize=18, fontweight='bold', color=COLORS['dark_blue'])\n",
    "\n",
    "# Plot 1: Color-coded table\n",
    "ax1.axis('tight')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Prepare table data\n",
    "table_data = []\n",
    "colors_list = []\n",
    "\n",
    "for _, row in predictions_2025_26_knn.iterrows():\n",
    "    rank = row['predicted_rank']\n",
    "    team = row['team']\n",
    "    expected = f\"{row['expected_position']:.2f}\"\n",
    "    \n",
    "    # Determine color\n",
    "    if rank <= 3:\n",
    "        color = COLORS['champions_league']\n",
    "        zone = \"Top 3\"\n",
    "    elif rank >= 16:\n",
    "        color = COLORS['relegation']\n",
    "        zone = \"Relegation\"\n",
    "    else:\n",
    "        color = COLORS['safe']\n",
    "        zone = \"Safe\"\n",
    "    \n",
    "    table_data.append([rank, team, expected, zone])\n",
    "    colors_list.append([color] * 4)\n",
    "\n",
    "table = ax1.table(cellText=table_data,\n",
    "                 colLabels=['Pos', 'Team', 'Expected Pos', 'Zone'],\n",
    "                 cellLoc='left',\n",
    "                 loc='center',\n",
    "                 cellColours=colors_list,\n",
    "                 colWidths=[0.1, 0.5, 0.2, 0.2])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header row\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor(COLORS['dark_blue'])\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax1.set_title('2025-26 Predicted Final Table\\n(KNN k=7)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Plot 2: Expected position bar chart\n",
    "ax2.barh(range(len(predictions_2025_26_knn)), \n",
    "        predictions_2025_26_knn['expected_position'],\n",
    "        color=[COLORS['champions_league'] if r <= 4 else \n",
    "               COLORS['relegation'] if r >= 16 else \n",
    "               COLORS['safe'] \n",
    "               for r in predictions_2025_26_knn['predicted_rank']],\n",
    "        edgecolor=COLORS['dark_blue'],\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8)\n",
    "\n",
    "ax2.set_yticks(range(len(predictions_2025_26_knn)))\n",
    "ax2.set_yticklabels(predictions_2025_26_knn['team'], fontsize=10)\n",
    "ax2.set_xlabel('Expected Position (Lower = Better)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Expected Position Values\\n(Uncertainty Range)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.invert_xaxis()  # Lower position = better\n",
    "ax2.invert_yaxis()  # Rank 1 at top\n",
    "ax2.grid(True, axis='x', alpha=0.3)\n",
    "ax2.axvline(x=3.5, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Top 3 cutoff')\n",
    "ax2.axvline(x=15.5, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Relegation cutoff')\n",
    "ax2.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca PREDICTION SUMMARY (KNN k=7)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\ud83c\udfc6 Predicted Champion: {predictions_2025_26_knn.iloc[0]['team']}\")\n",
    "print(f\"   Expected position: {predictions_2025_26_knn.iloc[0]['expected_position']:.2f}\")\n",
    "\n",
    "print(f\"\\n\u2b50 Predicted Top 3 (AFC Champions League):\")\n",
    "for i in range(3):\n",
    "    team = predictions_2025_26_knn.iloc[i]['team']\n",
    "    exp = predictions_2025_26_knn.iloc[i]['expected_position']\n",
    "    print(f\"   {i+1}. {team:25s} (exp: {exp:.2f})\")\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f  Predicted Relegation Zone:\")\n",
    "for i in range(15, 18):\n",
    "    team = predictions_2025_26_knn.iloc[i]['team']\n",
    "    exp = predictions_2025_26_knn.iloc[i]['expected_position']\n",
    "    print(f\"   {i+1}. {team:25s} (exp: {exp:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udca1 MODEL INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"  \u2022 KNN k=7 validation performance: 4/15 exact, 11/15 within \u00b13\")\n",
    "print(\"  \u2022 This model excels at identifying similar team patterns\")\n",
    "print(\"  \u2022 Expected positions show uncertainty range (e.g., 3.05 means ~3rd place)\")\n",
    "print(\"  \u2022 Teams with close expected positions may finish in different order\")\n",
    "print(\"  \u2022 Use this alongside Random Forest predictions for best insights\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Comparison and Model Evaluation\n",
    "\n",
    "Compare multiple machine learning algorithms to find the best performer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Limitations\n",
    "\n",
    "**Key Features:**\n",
    "- Most important features typically include: points, goal difference, and goals scored\n",
    "- These statistics historically correlate strongly with future performance\n",
    "\n",
    "**Model Choice:**\n",
    "- RandomForestClassifier is used for its ability to capture non-linear relationships\n",
    "- Moderate tree depth (max_depth=8) helps prevent overfitting\n",
    "- Class weighting handles the challenge of predicting 16+ different league positions\n",
    "\n",
    "**Data Coverage:**\n",
    "- Training data: Saudi Pro League seasons from 2018/19 through 2024/25\n",
    "- Prediction target: 2025/26 season final standings\n",
    "- Each season's team statistics serve as features for predicting the next season\n",
    "\n",
    "**Handling Promoted Teams:**\n",
    "- Teams promoted from the First Division lack historical SPL statistics\n",
    "- Default approach: assign features equal to the average of the previous season's bottom three teams\n",
    "- This provides a reasonable baseline but may not capture individual team strengths\n",
    "\n",
    "**Important Limitations:**\n",
    "- **Ignores transfers:** Major signings (especially in the Saudi League's recent recruitment drive) are not accounted for\n",
    "- **No manager changes:** Coaching changes and tactical shifts are not reflected\n",
    "- **Injuries:** Player availability and injury impacts are not included\n",
    "- **Form trends:** Recent momentum within a season is not captured by season-end statistics alone\n",
    "- **External factors:** Financial investments, stadium changes, and organizational changes are ignored\n",
    "- **Limited training data:** Only 6-7 seasons of data may not capture all possible scenarios\n",
    "\n",
    "**Conclusion:**\n",
    "These predictions are **statistical estimates based on historical performance patterns only**. They should be interpreted as one analytical perspective among many factors that influence league outcomes. Real-world results will depend heavily on factors not captured in this model, particularly given the Saudi Pro League's recent transformation and major player acquisitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing EXPANDED set: RF + KNN + Ensembles\n",
    "print(\"Testing EXPANDED set: RF + KNN + Ensembles\")\n",
    "print(\"Goal: MAXIMIZE 'within \u00b13 positions' performance\\n\")\n",
    "\n",
    "# Ensure validation files have correct paths\n",
    "print(\"Setting up file paths with 'data/' prefix...\")\n",
    "validation_files = [\n",
    "    'data/saudi_2018-19.csv',\n",
    "    'data/saudi_2019-20.csv', \n",
    "    'data/saudi_2020-21.csv',\n",
    "    'data/saudi_2021-22.csv',\n",
    "    'data/saudi_2022-23.csv',\n",
    "    'data/saudi_2023-24.csv'\n",
    "]\n",
    "print(\"Validation files list created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and load 2024-25 results\n",
    "print(\"Loading and preparing training data...\")\n",
    "X_val_train, y_val_train, latest_val_features = prepare_training_data(validation_files)\n",
    "\n",
    "# Load actual 2024-25 results for optimization\n",
    "print(\"Loading 2024-25 actual results...\")\n",
    "actual_2024_25_temp = pd.read_csv('data/saudi_2024-25.csv')\n",
    "actual_2024_25_temp = parse_match_results(actual_2024_25_temp)\n",
    "actual_2024_25_summary = summarise_season(actual_2024_25_temp)\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}